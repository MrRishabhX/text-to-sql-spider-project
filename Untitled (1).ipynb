{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QENTip6Pq4YB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "a187814a-55e9-40fe-a255-89d679873ed3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-907070862.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn5xSS4GrXK6"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J33IAIkhrXgS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#model loading\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "# Path to your model folder in Google Drive\n",
        "model_path = '/content/drive/MyDrive/TextToSQL_Project/t5-spider-finetuned-final'\n",
        "\n",
        "# Load model and tokenizer from the saved folder\n",
        "model = T5ForConditionalGeneration.from_pretrained(pretrained_model_name_or_path=model_path, local_files_only=True)\n",
        "tokenizer = T5Tokenizer.from_pretrained(pretrained_model_name_or_path=model_path, local_files_only=True)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e41f79c5"
      },
      "source": [
        "The previous attempts to load the model using `from_pretrained` with a local path resulted in an error because the method still attempted to validate the path as a Hugging Face repository ID.\n",
        "\n",
        "To reliably load the model and tokenizer from the local Google Drive folder, we will load the model configuration and state dictionary separately. This approach is more explicit for local loading and avoids the repository ID validation issue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O03_x_us7Ea"
      },
      "outputs": [],
      "source": [
        "def generate_sql(question):\n",
        "    input_text = \"translate English to SQL: \" + question\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=256,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return sql_query\n",
        "\n",
        "# Example usage\n",
        "print(generate_sql(\"List all customers with orders above 1000\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gP74I52un4m"
      },
      "outputs": [],
      "source": [
        "!pip install nltk rouge_score scikit-learn --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8raw3E5v13B"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the Spider validation split\n",
        "spider_dataset = load_dataset('spider')\n",
        "val_questions = spider_dataset['validation']['question']\n",
        "val_sql_references = spider_dataset['validation']['query']\n",
        "\n",
        "# Generate predictions for entire validation set\n",
        "predictions = []\n",
        "for question in tqdm(val_questions, desc=\"Generating SQL predictions\"):\n",
        "    pred_sql = generate_sql(question)\n",
        "    predictions.append(pred_sql)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2gH84FXFCQ5"
      },
      "outputs": [],
      "source": [
        "# Explore Database Domains\n",
        "# Analyze database diversity\n",
        "from collections import Counter\n",
        "def explore_database_domains(dataset_split):\n",
        "    db_domains = Counter()\n",
        "    db_questions = {}\n",
        "\n",
        "    for example in dataset_split:\n",
        "        db_id = example['db_id']\n",
        "        db_domains[db_id] += 1\n",
        "\n",
        "        if db_id not in db_questions:\n",
        "            db_questions[db_id] = []\n",
        "        db_questions[db_id].append(example['question'])\n",
        "\n",
        "    return db_domains, db_questions\n",
        "\n",
        "db_stats, db_questions = explore_database_domains(spider_dataset['train'])\n",
        "\n",
        "print(f\"\\n=== DATABASE DOMAIN ANALYSIS ===\")\n",
        "print(f\"Total unique databases: {len(db_stats)}\")\n",
        "print(f\"Average questions per database: {sum(db_stats.values()) / len(db_stats):.1f}\")\n",
        "\n",
        "# Show top 10 databases by question count\n",
        "print(\"\\nTop 10 databases by question count:\")\n",
        "for db_id, count in db_stats.most_common(10):\n",
        "    print(f\"- {db_id}: {count} questions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VmiGzogFT5p"
      },
      "outputs": [],
      "source": [
        "#Examine Query Complexity Distribution\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Analyze SQL complexity patterns\n",
        "def analyze_sql_complexity(dataset_split):\n",
        "    complexity_analysis = {\n",
        "        'joins': 0,\n",
        "        'nested_queries': 0,\n",
        "        'group_by': 0,\n",
        "        'order_by': 0,\n",
        "        'having': 0,\n",
        "        'union': 0\n",
        "    }\n",
        "\n",
        "    for example in dataset_split:\n",
        "        sql = example['query'].upper()\n",
        "        if 'JOIN' in sql:\n",
        "            complexity_analysis['joins'] += 1\n",
        "        if any(word in sql for word in ['SELECT', 'FROM']) and sql.count('SELECT') > 1:\n",
        "            complexity_analysis['nested_queries'] += 1\n",
        "        if 'GROUP BY' in sql:\n",
        "            complexity_analysis['group_by'] += 1\n",
        "        if 'ORDER BY' in sql:\n",
        "            complexity_analysis['order_by'] += 1\n",
        "        if 'HAVING' in sql:\n",
        "            complexity_analysis['having'] += 1\n",
        "        if 'UNION' in sql:\n",
        "            complexity_analysis['union'] += 1\n",
        "\n",
        "    return complexity_analysis\n",
        "\n",
        "# Analyze training set complexity\n",
        "train_complexity = analyze_sql_complexity(spider_dataset['train'])\n",
        "print(\"\\n=== SQL COMPLEXITY ANALYSIS ===\")\n",
        "for feature, count in train_complexity.items():\n",
        "    percentage = (count / len(spider_dataset['train'])) * 100\n",
        "    print(f\"{feature.replace('_', ' ').title()}: {count} queries ({percentage:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpwDcCVOBilr"
      },
      "source": [
        "# Evaluate Model Performance Using *Metrics*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHh5qzuQBqpr"
      },
      "outputs": [],
      "source": [
        "# Install necessary packages if not installed:\n",
        "# !pip install nltk rouge_score scikit-learn --quiet\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Prepare references and predictions\n",
        "references = [sql.strip().lower() for sql in val_sql_references]\n",
        "preds = [pred.strip().lower() for pred in predictions]\n",
        "\n",
        "# BLEU score (sentence-level with smoothing)\n",
        "smoothie = SmoothingFunction().method4\n",
        "bleu_scores = [\n",
        "    sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothie)\n",
        "    for ref, pred in zip(references, preds)\n",
        "]\n",
        "print(f\"Average BLEU score: {sum(bleu_scores) / len(bleu_scores):.4f}\")\n",
        "\n",
        "# ROUGE scores\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge1 = rouge2 = rougeL = 0\n",
        "for ref, pred in zip(references, preds):\n",
        "    scores = scorer.score(ref, pred)\n",
        "    rouge1 += scores['rouge1'].fmeasure\n",
        "    rouge2 += scores['rouge2'].fmeasure\n",
        "    rougeL += scores['rougeL'].fmeasure\n",
        "n = len(preds)\n",
        "print(f\"Avg ROUGE-1 F1: {rouge1 / n:.4f}\")\n",
        "print(f\"Avg ROUGE-2 F1: {rouge2 / n:.4f}\")\n",
        "print(f\"Avg ROUGE-L F1: {rougeL / n:.4f}\")\n",
        "\n",
        "# Token-level F1 score (macro)\n",
        "token_set = set()\n",
        "for ref, pred in zip(references, preds):\n",
        "    token_set.update(ref.split())\n",
        "    token_set.update(pred.split())\n",
        "\n",
        "mlb = MultiLabelBinarizer(classes=list(token_set))\n",
        "refs_bin = mlb.fit_transform([set(r.split()) for r in references])\n",
        "preds_bin = mlb.transform([set(p.split()) for p in preds])\n",
        "f1 = f1_score(refs_bin, preds_bin, average='macro')\n",
        "print(f\"Macro token-level F1 score: {f1:.4f}\")\n",
        "\n",
        "# Exact Match Accuracy\n",
        "exact_matches = [ref == pred for ref, pred in zip(references, preds)]\n",
        "print(f\"Exact Match Accuracy: {sum(exact_matches) / len(exact_matches):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CSZQyGQB25Q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame({\n",
        "    'question': val_questions,\n",
        "    'reference_sql': val_sql_references,\n",
        "    'predicted_sql': predictions\n",
        "})\n",
        "\n",
        "df_results.to_csv('validation_predictions.csv', index=False)\n",
        "print(\"Saved predictions to validation_predictions.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2me_Xz5HgUm"
      },
      "source": [
        "Step 1: Setup and Prepare Your Data for Retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEv2ozOVHiru"
      },
      "outputs": [],
      "source": [
        "# Example documents to index for retrieval\n",
        "documents = [\n",
        "    \"Customer table contains id, name, address, and order history details.\",\n",
        "    \"Orders table has order id, customer id, product id, quantity, and price.\",\n",
        "    \"Products table lists product id, name, category, and stock level.\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg7RN2PUHtIZ"
      },
      "source": [
        "Step 2: Create Document Embeddings with Sentence Transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6FkTQaqHo61"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers faiss-cpu --quiet\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')  # Fast and accurate embedding model\n",
        "\n",
        "# Compute embeddings for your knowledge base documents\n",
        "doc_embeddings = embedder.encode(documents, convert_to_numpy=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A0W89u4Iow4"
      },
      "source": [
        "Step 3: Build a FAISS Index for Efficient Vector Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncyzxThzHy21"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "embedding_dim = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(embedding_dim)  # L2 distance index\n",
        "index.add(doc_embeddings)\n",
        "\n",
        "print(f\"Indexed {index.ntotal} documents\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAnGCLk7I0WA"
      },
      "source": [
        "Step 4: Define Retrieval Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e__p8uzdIwdt"
      },
      "outputs": [],
      "source": [
        "def retrieve(query, k=2):\n",
        "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    retrieved_docs = [documents[idx] for idx in indices[0]]\n",
        "    return retrieved_docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGao99caI_Qy"
      },
      "source": [
        "Step 5: Load Your Fine-Tuned T5 Model and Tokenizer for Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugMG7s8RI3Zc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "\n",
        "model_path = '/content/drive/MyDrive/TextToSQL_Project/t5-spider-finetuned-final'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path, local_files_only=True).to(device)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path, local_files_only=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Scn6DX25JbaR"
      },
      "source": [
        "Step 6: Combine Retriever and Generator in RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMJGCTe3JRIr"
      },
      "outputs": [],
      "source": [
        "def rag_generate_sql(question, k=1, max_gen_len=150):\n",
        "    context_docs = retrieve(question, k=k)\n",
        "    print(\"Retrieved docs:\", context_docs)\n",
        "\n",
        "    context_text = \" ; \".join(context_docs)\n",
        "    input_text = f\"translate English to SQL: {question} Context: {context_text} ###\"\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512).to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=max_gen_len,\n",
        "        num_beams=4,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=3,\n",
        "    )\n",
        "    generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_sql\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_je0FbrlJmGu"
      },
      "source": [
        "Step 7: Test Your RAG Pipeline\n",
        "python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xieFGVl3JhWT"
      },
      "outputs": [],
      "source": [
        "sample_question = \"List all customers\"\n",
        "print(\"Generated SQL:\")\n",
        "print(rag_generate_sql(sample_question))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrMvXCdOJoS0"
      },
      "outputs": [],
      "source": [
        "sample_question = \"List all customers with orders above 1000\"\n",
        "print(\"Generated SQL:\")\n",
        "print(rag_generate_sql(sample_question))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO30sFinKWey"
      },
      "outputs": [],
      "source": [
        "sample_question = \"Give me name of the department\"\n",
        "print(\"Generated SQL:\")\n",
        "print(rag_generate_sql(sample_question))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-be6Rc8K8bf"
      },
      "source": [
        "Step 8: Evaluate the RAG System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ouj_9_HzKIUs"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load validation data questions and references\n",
        "spider_dataset = load_dataset('spider')\n",
        "val_questions = spider_dataset['validation']['question']\n",
        "\n",
        "val_references = spider_dataset['validation']['query']\n",
        "\n",
        "predictions = []\n",
        "for question in tqdm(val_questions, desc=\"Generating RAG SQL predictions\"):\n",
        "    pred_sql = rag_generate_sql(question)\n",
        "    predictions.append(pred_sql)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "evaluation metrics"
      ],
      "metadata": {
        "id": "4ujQH0eNpVs3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqkDJHrzK_Wj"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Prepare references and predictions (lowercase and stripped)\n",
        "references = [sql.strip().lower() for sql in val_references]\n",
        "preds = [pred.strip().lower() for pred in predictions]\n",
        "\n",
        "# BLEU Score with smoothing\n",
        "smoothie = SmoothingFunction().method4\n",
        "bleu_scores = [\n",
        "    sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothie)\n",
        "    for ref, pred in zip(references, preds)\n",
        "]\n",
        "print(f\"Average BLEU score: {sum(bleu_scores) / len(bleu_scores):.4f}\")\n",
        "\n",
        "# ROUGE Scores\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge1 = rouge2 = rougeL = 0\n",
        "for ref, pred in zip(references, preds):\n",
        "    scores = scorer.score(ref, pred)\n",
        "    rouge1 += scores['rouge1'].fmeasure\n",
        "    rouge2 += scores['rouge2'].fmeasure\n",
        "    rougeL += scores['rougeL'].fmeasure\n",
        "n = len(preds)\n",
        "print(f\"Avg ROUGE-1 F1: {rouge1 / n:.4f}\")\n",
        "print(f\"Avg ROUGE-2 F1: {rouge2 / n:.4f}\")\n",
        "print(f\"Avg ROUGE-L F1: {rougeL / n:.4f}\")\n",
        "\n",
        "# Token-level F1 Score\n",
        "all_ref_tokens = [ref.split() for ref in references]\n",
        "all_pred_tokens = [pred.split() for pred in preds]\n",
        "tokens = set(sum(all_ref_tokens, []) + sum(all_pred_tokens, []))\n",
        "mlb = MultiLabelBinarizer(classes=list(tokens))\n",
        "refs_bin = mlb.fit_transform([set(r) for r in all_ref_tokens])\n",
        "preds_bin = mlb.transform([set(p) for p in all_pred_tokens])\n",
        "f1 = f1_score(refs_bin, preds_bin, average='macro')\n",
        "print(f\"Macro token-level F1 score: {f1:.4f}\")\n",
        "\n",
        "# Exact Match Accuracy\n",
        "exact_matches = [ref == pred for ref, pred in zip(references, preds)]\n",
        "print(f\"Exact Match Accuracy: {sum(exact_matches) / len(exact_matches):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_results = pd.DataFrame({\n",
        "    'question': val_questions,\n",
        "    'reference_sql': val_references,\n",
        "    'predicted_sql': predictions\n",
        "})\n",
        "\n",
        "df_results.to_csv('rag_validation_predictions.csv', index=False)\n",
        "print(\"Saved validation predictions to rag_validation_predictions.csv\")\n"
      ],
      "metadata": {
        "id": "sxw6nzQJpktz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install all required packages\n",
        "!pip install transformers datasets sentence-transformers faiss-cpu rouge_score nltk scikit-learn tqdm --quiet\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# --- Set device ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- Step 1: Load fine-tuned T5 model and tokenizer ---\n",
        "\n",
        "model_path = '/content/drive/MyDrive/TextToSQL_Project/t5-spider-finetuned-final'  # CHANGE to your model directory\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path, local_files_only=True)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "model.to(device)\n",
        "print(\"Loaded fine-tuned T5 model and tokenizer\")\n",
        "\n",
        "# --- Step 2: Prepare retrieval knowledge base and build improved retriever ---\n",
        "\n",
        "# Replace this list with your actual schema/docs or SQL examples as retrieval corpus\n",
        "documents = [\n",
        "    \"Customer table with id, name, address, and order details.\",\n",
        "    \"Orders table contains order_id, customer_id, product_id, quantity, price.\",\n",
        "    \"Products table contains product_id, name, category, and stock quantity.\",\n",
        "    \"Customers who have orders greater than 1000.\",\n",
        "    \"Product categories include electronics, furniture, clothing, and toys.\"\n",
        "]\n",
        "\n",
        "print(\"Loading Sentence Transformer embedder...\")\n",
        "embedder = SentenceTransformer('all-mpnet-base-v2')  # Stronger embedding model\n",
        "\n",
        "print(\"Computing document embeddings...\")\n",
        "doc_embeddings = embedder.encode(documents, convert_to_numpy=True)\n",
        "\n",
        "embedding_dim = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(embedding_dim)\n",
        "index.add(doc_embeddings)\n",
        "print(f\"FAISS index built with {index.ntotal} documents for retrieval.\")\n",
        "\n",
        "# --- Step 3: Retrieval function ---\n",
        "\n",
        "def retrieve(query, k=2):\n",
        "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    retrieved = [documents[i] for i in indices[0]]\n",
        "    return retrieved\n",
        "\n",
        "# --- Step 4: Further fine-tune the T5 model ---\n",
        "\n",
        "# Load Spider dataset for training & validation\n",
        "spider_dataset = load_dataset('spider')\n",
        "\n",
        "# Preprocessing function (adjust if needed)\n",
        "max_input_length = 512\n",
        "max_target_length = 256\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [\"translate English to SQL: \" + q.strip() for q in examples[\"question\"]]\n",
        "    targets = [sql.strip() for sql in examples[\"query\"]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "print(\"Tokenizing training and validation data...\")\n",
        "tokenized_train = spider_dataset[\"train\"].map(preprocess_function, batched=True)\n",
        "tokenized_validation = spider_dataset[\"validation\"].map(preprocess_function, batched=True)\n",
        "\n",
        "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "tokenized_validation.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./t5-finetuned-continued\",\n",
        "    num_train_epochs=3,              # Increase epochs for further fine-tuning\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,              # Lower LR for stable tuning\n",
        "    # eval_strategy=\"steps\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    logging_steps=100,\n",
        "    fp16=True,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_validation,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Starting continued fine-tuning...\")\n",
        "trainer.train()\n",
        "trainer.save_model('./t5-finetuned-continued')\n",
        "tokenizer.save_pretrained('./t5-finetuned-continued')\n",
        "save_path = '/content/drive/MyDrive/t5-finetuned-continued'\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(\"Fine-tuning complete and model saved.\")\n",
        "\n",
        "model.save_pretrained(\"/content/drive/MyDrive/TextToSQL_Project/t5-finetuned-continued\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/TextToSQL_Project/t5-finetuned-continued\")\n",
        "\n",
        "\n",
        "# --- Step 5: Retrieval-Augmented Generation (RAG) function with improved prompt and generation controls ---\n",
        "\n",
        "def rag_generate_sql(question, k=2, max_gen_len=150):\n",
        "    retrieved_docs = retrieve(question, k=k)\n",
        "    print(f\"Retrieved {k} documents for query: {question}\\n- \" + \"\\n- \".join(retrieved_docs))\n",
        "\n",
        "    context_text = \" ; \".join(retrieved_docs)\n",
        "    input_text = f\"translate English to SQL: {question} Context: {context_text} ###\"\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512).to(device)\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=max_gen_len,\n",
        "        num_beams=5,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=3,\n",
        "        repetition_penalty=2.0,\n",
        "        length_penalty=1.0,\n",
        "    )\n",
        "\n",
        "    generated_sql = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return generated_sql\n",
        "\n",
        "# --- Step 6: Generate predictions on Spider validation set ---\n",
        "\n",
        "val_questions = spider_dataset['validation']['question']\n",
        "val_references = spider_dataset['validation']['query']\n",
        "\n",
        "print(\"Generating RAG-enhanced SQL predictions for validation set (this will take some time)...\")\n",
        "predictions = []\n",
        "for q in tqdm(val_questions):\n",
        "    pred_sql = rag_generate_sql(q, k=2, max_gen_len=150)\n",
        "    predictions.append(pred_sql)\n",
        "\n",
        "# --- Step 7: Evaluate the predictions ---\n",
        "\n",
        "# Prepare for evaluation\n",
        "references = [r.strip().lower() for r in val_references]\n",
        "preds = [p.strip().lower() for p in predictions]\n",
        "\n",
        "# BLEU with smoothing\n",
        "smoothie = SmoothingFunction().method4\n",
        "bleu_scores = [\n",
        "    sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothie)\n",
        "    for ref, pred in zip(references, preds)\n",
        "]\n",
        "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "# ROUGE\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge1_sum = rouge2_sum = rougeL_sum = 0\n",
        "for ref, pred in zip(references, preds):\n",
        "    score = scorer.score(ref, pred)\n",
        "    rouge1_sum += score['rouge1'].fmeasure\n",
        "    rouge2_sum += score['rouge2'].fmeasure\n",
        "    rougeL_sum += score['rougeL'].fmeasure\n",
        "n = len(preds)\n",
        "avg_rouge1 = rouge1_sum / n\n",
        "avg_rouge2 = rouge2_sum / n\n",
        "avg_rougeL = rougeL_sum / n\n",
        "\n",
        "# Token-level Macro F1\n",
        "ref_tokens = [ref.split() for ref in references]\n",
        "pred_tokens = [pred.split() for pred in preds]\n",
        "token_set = set(sum(ref_tokens, []) + sum(pred_tokens, []))\n",
        "mlb = MultiLabelBinarizer(classes=list(token_set))\n",
        "ref_bin = mlb.fit_transform([set(r) for r in ref_tokens])\n",
        "pred_bin = mlb.transform([set(p) for p in pred_tokens])\n",
        "macro_f1 = f1_score(ref_bin, pred_bin, average='macro')\n",
        "\n",
        "# Exact Match\n",
        "exact_matches = [ref == pred for ref, pred in zip(references, preds)]\n",
        "exact_match = sum(exact_matches) / len(exact_matches)\n",
        "\n",
        "print(\"\\n==== Evaluation Metrics ====\")\n",
        "print(f\"Average BLEU Score       : {avg_bleu:.4f}\")\n",
        "print(f\"Average ROUGE-1 F1       : {avg_rouge1:.4f}\")\n",
        "print(f\"Average ROUGE-2 F1       : {avg_rouge2:.4f}\")\n",
        "print(f\"Average ROUGE-L F1       : {avg_rougeL:.4f}\")\n",
        "print(f\"Macro Token-level F1     : {macro_f1:.4f}\")\n",
        "print(f\"Exact Match Accuracy     : {exact_match:.4f}\")\n",
        "\n",
        "# --- Step 8: Save results for inspection ---\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'question': val_questions,\n",
        "    'reference_sql': val_references,\n",
        "    'predicted_sql': predictions\n",
        "})\n",
        "\n",
        "results_df.to_csv('rag_validation_predictions.csv', index=False)\n",
        "print(\"Saved RAG validation results to rag_validation_predictions.csv\")\n"
      ],
      "metadata": {
        "id": "5MhDdQMB_e3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install requirements\n",
        "!pip install transformers datasets sentence-transformers faiss-cpu rouge_score nltk scikit-learn tqdm sqlparse --quiet\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import sqlite3\n",
        "import glob\n",
        "import sqlparse\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "### 1. Load fine-tuned T5 model ###\n",
        "model_path = '/content/drive/MyDrive/TextToSQL_Project/t5-spider-finetuned-final'  # change if needed\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path, local_files_only=True)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "model.to(device)\n",
        "print(\"Loaded fine-tuned T5 model and tokenizer\")\n",
        "\n",
        "### 2. Build enriched retrieval corpus ###\n",
        "spider_db_dir = '/content/spider/database'\n",
        "\n",
        "retrieval_documents = []\n",
        "for db_dir in glob.glob(f\"{spider_db_dir}/*\"):\n",
        "    db_name = os.path.basename(db_dir)\n",
        "    sqlite_files = [f for f in os.listdir(db_dir) if f.endswith('.sqlite')]\n",
        "    for sqlite_file in sqlite_files:\n",
        "        db_path = os.path.join(db_dir, sqlite_file)\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
        "        tables = [row[0] for row in cursor.fetchall()]\n",
        "        for table in tables:\n",
        "            cursor.execute(f\"PRAGMA table_info({table})\")\n",
        "            cols = cursor.fetchall()\n",
        "            col_desc = \", \".join([f\"{c[1]} ({c[2]})\" for c in cols])\n",
        "            retrieval_documents.append(f\"Database: {db_name}. Table: {table}. Columns: {col_desc}.\")\n",
        "        conn.close()\n",
        "\n",
        "print(f\"Extracted {len(retrieval_documents)} schema descriptions from Spider.\")\n",
        "\n",
        "# Extra optional schemas\n",
        "extra_documents = [\n",
        "    \"Table: Accounts. Columns: account_id (PK), customer_id (FK), account_type, balance, currency, open_date.\",\n",
        "    \"Table: Students. Columns: student_id (PK), name, email, enrollment_year, major.\",\n",
        "    \"Table: Courses. Columns: course_id (PK), course_name, department, credits.\",\n",
        "    \"Relationship: Each account can have multiple transactions. Students can enroll in multiple courses.\",\n",
        "]\n",
        "retrieval_documents += extra_documents\n",
        "\n",
        "### 3. Build embeddings and FAISS index ###\n",
        "print(\"Embedding and indexing retrieval corpus...\")\n",
        "embedder = SentenceTransformer('all-mpnet-base-v2')\n",
        "doc_embeddings = embedder.encode(retrieval_documents, convert_to_numpy=True)\n",
        "embedding_dim = doc_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(embedding_dim)\n",
        "index.add(doc_embeddings)\n",
        "print(f\"FAISS index built with {index.ntotal} enriched documents.\")\n",
        "\n",
        "def retrieve(query, k=5):\n",
        "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "    return [retrieval_documents[i] for i in indices[0]]\n",
        "\n",
        "def find_schema_context(question, k=5):\n",
        "    return \"\\n\".join(retrieve(question, k))\n",
        "\n",
        "### 4. Prepare Spider dataset with improved prompt ###\n",
        "spider_dataset = load_dataset(\"xlangai/spider\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    augmented_inputs = []\n",
        "    for q in examples[\"question\"]:\n",
        "        schema_ctx = find_schema_context(q, k=5)\n",
        "        augmented_inputs.append(\n",
        "            f\"Given the database schema below, write the correct SQL query to answer the question.\\n\\n\"\n",
        "            f\"Schema:\\n{schema_ctx}\\n\\nQuestion:\\n{q}\\n\\nSQL:\"\n",
        "        )\n",
        "    targets = [sql.strip() for sql in examples[\"query\"]]\n",
        "    model_inputs = tokenizer(augmented_inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "print(\"Tokenizing Spider train/validation data with improved prompts...\")\n",
        "tokenized_train = spider_dataset[\"train\"].map(preprocess_function, batched=True)\n",
        "tokenized_validation = spider_dataset[\"validation\"].map(preprocess_function, batched=True)\n",
        "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "tokenized_validation.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "\n",
        "\n",
        "#New Code\n",
        "from itertools import product\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    \"learning_rate\": [2e-5, 3e-5],\n",
        "    \"per_device_train_batch_size\": [8, 16],\n",
        "    \"num_train_epochs\": [3, 5],\n",
        "    \"warmup_ratio\": [0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Create all combinations\n",
        "keys, values = zip(*param_grid.items())\n",
        "param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
        "\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import numpy as np\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, params in enumerate(param_combinations):\n",
        "    print(f\"Running grid search combination {i+1}/{len(param_combinations)}: {params}\")\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./model_grid_{i}\",\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=500,\n",
        "        save_steps=500,\n",
        "        save_total_limit=1,\n",
        "        learning_rate=params[\"learning_rate\"],\n",
        "        per_device_train_batch_size=params[\"per_device_train_batch_size\"],\n",
        "        per_device_eval_batch_size=params[\"per_device_train_batch_size\"],\n",
        "        num_train_epochs=params[\"num_train_epochs\"],\n",
        "        warmup_ratio=params[\"warmup_ratio\"],\n",
        "        load_best_model_at_end=True,\n",
        "        logging_steps=100,\n",
        "        fp16=True,\n",
        "        report_to=None\n",
        "    )\n",
        "\n",
        "\n",
        "    trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_validation,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "    # Train\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate\n",
        "    metrics = trainer.evaluate()\n",
        "    metrics['params'] = params\n",
        "    results.append(metrics)\n",
        "\n",
        "### 5. Fine-tune with better settings ###\n",
        "\n",
        "\n",
        "print(metrics)\n",
        "\n",
        "\n",
        "\n",
        "#Ends New Code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./t5-finetuned-rag\",\n",
        "#     num_train_epochs=10,\n",
        "#     per_device_train_batch_size=8,\n",
        "#     per_device_eval_batch_size=8,\n",
        "#     learning_rate=2e-5,\n",
        "#     warmup_ratio=0.1,\n",
        "#     eval_strategy=\"steps\",\n",
        "#     eval_steps=500,\n",
        "#     save_steps=500,\n",
        "#     save_total_limit=2,\n",
        "#     load_best_model_at_end=True,\n",
        "#     logging_steps=100,\n",
        "#     fp16=True,\n",
        "#     report_to=None\n",
        "# )\n",
        "\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=tokenized_train,\n",
        "#     eval_dataset=tokenized_validation,\n",
        "#     tokenizer=tokenizer,\n",
        "# )\n",
        "\n",
        "# print(\"Starting improved retrieval-augmented fine-tuning...\")\n",
        "# trainer.train()\n",
        "trainer.save_model('./t5-finetuned-rag')\n",
        "tokenizer.save_pretrained('./t5-finetuned-rag')\n",
        "\n",
        "### 6. Reload for inference ###\n",
        "model = T5ForConditionalGeneration.from_pretrained('./t5-finetuned-rag').to(device)\n",
        "tokenizer = T5Tokenizer.from_pretrained('./t5-finetuned-rag')\n",
        "\n",
        "def rag_generate_sql(question, k=5, max_gen_len=180):\n",
        "    schema_ctx = find_schema_context(question, k)\n",
        "    input_text = (\n",
        "        f\"Given the database schema below, write the correct SQL query to answer the question.\\n\\n\"\n",
        "        f\"Schema:\\n{schema_ctx}\\n\\nQuestion:\\n{question}\\n\\nSQL:\"\n",
        "    )\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=1024).to(device)\n",
        "    output_ids = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=max_gen_len,\n",
        "        num_beams=8,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=2,\n",
        "        repetition_penalty=1.2,\n",
        "        length_penalty=1.0,\n",
        "    )\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "### 7. Evaluation with canonicalization ###\n",
        "val_questions = spider_dataset['validation']['question']\n",
        "val_references = spider_dataset['validation']['query']\n",
        "\n",
        "print(\"Generating improved RAG SQL predictions for validation set...\")\n",
        "predictions = [rag_generate_sql(q) for q in tqdm(val_questions)]\n",
        "\n",
        "def canonicalize_sql(sql):\n",
        "    try:\n",
        "        return \" \".join(sqlparse.format(sql, reindent=True, keyword_case='upper').split())\n",
        "    except:\n",
        "        return sql.strip()\n",
        "\n",
        "references = [canonicalize_sql(r.lower()) for r in val_references]\n",
        "preds = [canonicalize_sql(p.lower()) for p in predictions]\n",
        "\n",
        "smoothie = SmoothingFunction().method4\n",
        "bleu_scores = [sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothie)\n",
        "               for ref, pred in zip(references, preds)]\n",
        "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge1_sum = rouge2_sum = rougeL_sum = 0\n",
        "for ref, pred in zip(references, preds):\n",
        "    score = scorer.score(ref, pred)\n",
        "    rouge1_sum += score['rouge1'].fmeasure\n",
        "    rouge2_sum += score['rouge2'].fmeasure\n",
        "    rougeL_sum += score['rougeL'].fmeasure\n",
        "\n",
        "n = len(preds)\n",
        "avg_rouge1 = rouge1_sum / n\n",
        "avg_rouge2 = rouge2_sum / n\n",
        "avg_rougeL = rougeL_sum / n\n",
        "\n",
        "ref_tokens = [ref.split() for ref in references]\n",
        "pred_tokens = [pred.split() for pred in preds]\n",
        "token_set = set(sum(ref_tokens, []) + sum(pred_tokens, []))\n",
        "mlb = MultiLabelBinarizer(classes=list(token_set))\n",
        "ref_bin = mlb.fit_transform([set(r) for r in ref_tokens])\n",
        "pred_bin = mlb.transform([set(p) for p in pred_tokens])\n",
        "macro_f1 = f1_score(ref_bin, pred_bin, average='macro')\n",
        "\n",
        "exact_matches = [ref == pred for ref, pred in zip(references, preds)]\n",
        "exact_match = sum(exact_matches) / len(exact_matches)\n",
        "\n",
        "print(\"\\n==== Improved Evaluation Metrics ====\")\n",
        "print(f\"Average BLEU Score     : {avg_bleu:.4f}\")\n",
        "print(f\"Average ROUGE-1 F1     : {avg_rouge1:.4f}\")\n",
        "print(f\"Average ROUGE-2 F1     : {avg_rouge2:.4f}\")\n",
        "print(f\"Average ROUGE-L F1     : {avg_rougeL:.4f}\")\n",
        "print(f\"Macro Token-level F1   : {macro_f1:.4f}\")\n",
        "print(f\"Exact Match Accuracy   : {exact_match:.4f}\")\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'question': val_questions,\n",
        "    'reference_sql': val_references,\n",
        "    'predicted_sql': predictions\n",
        "})\n",
        "results_df.to_csv('rag_validation_predictions.csv', index=False)\n",
        "print(\"Saved improved RAG validation results to rag_validation_predictions.csv.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "sEobNjFdHhlB",
        "outputId": "246f1db8-254f-4eb5-bab4-fd65f6102977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 85.38 MiB is free. Process 143536 has 22.07 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 6.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2400898366.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded fine-tuned T5 model and tokenizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4331\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4332\u001b[0m                 )\n\u001b[0;32m-> 4333\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 85.38 MiB is free. Process 143536 has 22.07 GiB memory in use. Of the allocated memory 21.83 GiB is allocated by PyTorch, and 6.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets sentence-transformers faiss-cpu rouge_score nltk scikit-learn tqdm sqlparse --quiet\n",
        "\n",
        "# Optional: mount Drive if needed\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, glob, sqlite3, sqlparse\n",
        "import torch, faiss, numpy as np, pandas as pd, nltk\n",
        "from tqdm import tqdm\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[INFO] Using device: {device}\")\n",
        "\n",
        "# ===== PATHS (EDIT THESE) =====\n",
        "model_path = \"/content/drive/MyDrive/TextToSQL_Project/t5-spider-finetuned-final\"\n",
        "spider_db_dir = \"/content/drive/MyDrive/TextToSQL_Project/spider/database\"\n",
        "\n",
        "# ===== LOAD MODEL =====\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path, local_files_only=True).to(device)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "print(\"[INFO] Loaded model and tokenizer\")\n",
        "\n",
        "# ===== BUILD RETRIEVAL CORPUS =====\n",
        "retrieval_documents = []\n",
        "def safe_exec(cursor, query):\n",
        "    try:\n",
        "        cursor.execute(query)\n",
        "        return cursor.fetchall()\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "for db_dir in glob.glob(f\"{spider_db_dir}/*\"):\n",
        "    db_name = os.path.basename(db_dir)\n",
        "    sqlite_files = [f for f in os.listdir(db_dir) if f.endswith(\".sqlite\")]\n",
        "    for sqlite_file in sqlite_files:\n",
        "        db_path = os.path.join(db_dir, sqlite_file)\n",
        "        try:\n",
        "            conn = sqlite3.connect(db_path)\n",
        "            cursor = conn.cursor()\n",
        "            tables = [row[0] for row in safe_exec(cursor, \"SELECT name FROM sqlite_master WHERE type='table'\")]\n",
        "            for table in tables:\n",
        "                cols = safe_exec(cursor, f\"PRAGMA table_info({table})\")\n",
        "                col_desc = \", \".join([f\"{c[1]} ({c[2]})\" for c in cols])\n",
        "                retrieval_documents.append(f\"Database: {db_name}. Table: {table}. Columns: {col_desc}.\")\n",
        "            for table in tables:\n",
        "                fks = safe_exec(cursor, f\"PRAGMA foreign_key_list({table})\")\n",
        "                for fk in fks:\n",
        "                    _, _, ref_table, from_col, to_col, *_ = fk\n",
        "                    retrieval_documents.append(f\"Join: {db_name}.{table}.{from_col} -> {db_name}.{ref_table}.{to_col}\")\n",
        "            conn.close()\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Failed to parse {db_path}: {e}\")\n",
        "\n",
        "retrieval_documents += [\n",
        "    \"Relationship: Students.student_id -> Enrollments.student_id; Enrollments.course_id -> Courses.course_id.\",\n",
        "    \"Relationship: Accounts.account_id -> Transactions.account_id.\"\n",
        "]\n",
        "print(f\"[INFO] Built retrieval corpus with {len(retrieval_documents)} documents\")\n",
        "\n",
        "# ===== EMBED AND INDEX =====\n",
        "embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "doc_embeddings = embedder.encode(retrieval_documents, convert_to_numpy=True)\n",
        "index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
        "index.add(doc_embeddings)\n",
        "print(f\"[INFO] FAISS index built with {index.ntotal} documents\")\n",
        "\n",
        "def retrieve(query, k=2):\n",
        "    query_emb = embedder.encode([query], convert_to_numpy=True)\n",
        "    _, indices = index.search(query_emb, k)\n",
        "    return [retrieval_documents[i] for i in indices[0]]\n",
        "\n",
        "def find_schema_context(question, k=2):\n",
        "    return \" ; \".join(retrieve(question, k))\n",
        "\n",
        "# ===== LOAD DATASET =====\n",
        "spider = load_dataset(\"xlangai/spider\")\n",
        "train_split = spider[\"train\"]\n",
        "val_split = spider[\"validation\"]\n",
        "\n",
        "# ===== PREPROCESS =====\n",
        "def preprocess_function(examples, k=2):\n",
        "    inputs = []\n",
        "    for q in examples[\"question\"]:\n",
        "        schema_ctx = find_schema_context(q, k)\n",
        "        inputs.append(f\"translate English to SQL: Question: {q.strip()} Schema: {schema_ctx} SQL:\")\n",
        "    targets = [sql.strip() for sql in examples[\"query\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "print(\"[INFO] Tokenizing dataset...\")\n",
        "tokenized_train = train_split.map(lambda x: preprocess_function(x, k=2), batched=True)\n",
        "tokenized_val = val_split.map(lambda x: preprocess_function(x, k=2), batched=True)\n",
        "tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "tokenized_val.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# ===== TRAINING =====\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/TextToSQL_Project/t5-finetuned-rag-compact\",\n",
        "    num_train_epochs=10,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=500,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=800,\n",
        "    save_steps=800,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,\n",
        "    logging_steps=200,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    processing_class=tokenizer,  # New parameter name\n",
        ")\n",
        "\n",
        "\n",
        "print(\"[INFO] Starting training...\")\n",
        "trainer.train()\n",
        "trainer.save_model(\"/content/drive/MyDrive/TextToSQL_Project/t5-finetuned-rag-compact\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/TextToSQL_Project/t5-finetuned-rag-compact\")\n",
        "\n",
        "# ===== RELOAD MODEL =====\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/TextToSQL_Project/t5-finetuned-rag-compact\").to(device)\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"/content/drive/MyDrive/TextToSQL_Project/t5-finetuned-rag-compact\")\n",
        "\n",
        "# ===== SQL UTILITIES =====\n",
        "def normalize_sql(sql):\n",
        "    try:\n",
        "        return sqlparse.format(sql, keyword_case=\"upper\", reindent=True, strip_comments=True).strip()\n",
        "    except:\n",
        "        return sql.strip().upper()\n",
        "\n",
        "def execute_sql_on_db(db_path, sql):\n",
        "    try:\n",
        "        conn = sqlite3.connect(db_path)\n",
        "        conn.text_factory = str\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(sql)\n",
        "        result = cursor.fetchall()\n",
        "        conn.close()\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        return f\"[ERROR] {e}\"\n",
        "\n",
        "# ===== MAP DB PATHS =====\n",
        "db_paths = {}\n",
        "for db_dir in glob.glob(f\"{spider_db_dir}/*\"):\n",
        "    db_name = os.path.basename(db_dir)\n",
        "    sqlite_files = [f for f in os.listdir(db_dir) if f.endswith(\".sqlite\")]\n",
        "    if sqlite_files:\n",
        "        db_paths[db_name] = os.path.join(db_dir, sqlite_files[0])\n",
        "\n",
        "# ===== GENERATION FUNCTION =====\n",
        "def generate_sql_for_question(question, k, num_beams, length_penalty, repetition_penalty=1.5, max_len=180):\n",
        "    schema_ctx = find_schema_context(question, k)\n",
        "    input_text = f\"translate English to SQL: Question: {question.strip()} Schema: {schema_ctx} SQL:\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512).to(device)\n",
        "    output_ids = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=max_len,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=3,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        length_penalty=length_penalty,\n",
        "    )\n",
        "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# ===== PARAMETER SWEEP =====\n",
        "val_questions = val_split['question']\n",
        "val_references = val_split['query']\n",
        "val_db_ids = val_split['db_id']\n",
        "\n",
        "test_settings = [\n",
        "    {\"k\": 2, \"num_beams\": 6, \"length_penalty\": 1.2},\n",
        "    {\"k\": 2, \"num_beams\": 5, \"length_penalty\": 1.0},\n",
        "    {\"k\": 3, \"num_beams\": 6, \"length_penalty\": 1.2},\n",
        "]\n",
        "\n",
        "best_exec_acc = -1.0\n",
        "best_bleu = -1.0\n",
        "best_preds = None\n",
        "best_refs = None\n",
        "best_setting = None\n",
        "\n",
        "print(\"[INFO] Running parameter sweep...\")\n",
        "for setting in test_settings:\n",
        "    predictions = []\n",
        "    references_norm = []\n",
        "    exec_correct = 0\n",
        "    total_exec = 0\n",
        "\n",
        "    for q, gold_sql, db_id in tqdm(zip(val_questions, val_references, val_db_ids), total=len(val_questions)):\n",
        "        pred_sql = generate_sql_for_question(q, setting[\"k\"], setting[\"num_beams\"], setting[\"length_penalty\"])\n",
        "        pred_sql_norm = normalize_sql(pred_sql)\n",
        "        gold_sql_norm = normalize_sql(gold_sql)\n",
        "\n",
        "        predictions.append(pred_sql_norm)\n",
        "        references_norm.append(gold_sql_norm)\n",
        "\n",
        "        db_path = db_paths.get(db_id)\n",
        "        if db_path:\n",
        "            gold_result = execute_sql_on_db(db_path, gold_sql_norm)\n",
        "            pred_result = execute_sql_on_db(db_path, pred_sql_norm)\n",
        "            if not isinstance(gold_result, str) and not isinstance(pred_result, str):\n",
        "                total_exec += 1\n",
        "                if gold_result == pred_result:\n",
        "                    exec_correct += 1\n",
        "\n",
        "    exec_acc = exec_correct / total_exec if total_exec > 0 else 0.0\n",
        "\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleu_scores = [sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothie)\n",
        "                   for ref, pred in zip(references_norm, predictions)]\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "    print(f\"[INFO] Settings {setting} -> ExecAcc={exec_acc:.4f} on {total_exec} queries, BLEU={avg_bleu:.4f}\")\n",
        "\n",
        "    if (exec_acc > best_exec_acc) or (exec_acc == best_exec_acc and avg_bleu > best_bleu):\n",
        "        best_exec_acc = exec_acc\n",
        "        best_bleu = avg_bleu\n",
        "        best_preds = predictions\n",
        "        best_refs = references_norm\n",
        "        best_setting = setting\n",
        "\n",
        "# ===== FINAL EVALUATION =====\n",
        "print(f\"\\n[RESULT] Best settings: {best_setting}\")\n",
        "print(f\"Best Execution Accuracy: {best_exec_acc:.4f}\")\n",
        "print(f\"Best BLEU: {best_bleu:.4f}\")\n",
        "\n",
        "# ROUGE scores\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge1_sum = rouge2_sum = rougeL_sum = 0\n",
        "for ref, pred in zip(best_refs, best_preds):\n",
        "    scores = scorer.score(ref, pred)\n",
        "    rouge1_sum += scores['rouge1'].fmeasure\n",
        "    rouge2_sum += scores['rouge2'].fmeasure\n",
        "    rougeL_sum += scores['rougeL'].fmeasure\n",
        "\n",
        "n = len(best_preds)\n",
        "avg_rouge1 = rouge1_sum / n\n",
        "avg_rouge2 = rouge2_sum / n\n",
        "avg_rougeL = rougeL_sum / n\n",
        "\n",
        "# Macro F1\n",
        "ref_tokens = [ref.split() for ref in best_refs]\n",
        "pred_tokens = [pred.split() for pred in best_preds]\n",
        "all_tokens = set()\n",
        "for tokens in ref_tokens + pred_tokens:\n",
        "    all_tokens.update(tokens)\n",
        "\n",
        "mlb = MultiLabelBinarizer(classes=list(all_tokens))\n",
        "ref_binary = mlb.fit_transform([set(tokens) for tokens in ref_tokens])\n",
        "pred_binary = mlb.transform([set(tokens) for tokens in pred_tokens])\n",
        "macro_f1 = f1_score(ref_binary, pred_binary, average='macro')\n",
        "\n",
        "# Exact Match\n",
        "exact_matches = [ref == pred for ref, pred in zip(best_refs, best_preds)]\n",
        "exact_match = sum(exact_matches) / len(exact_matches)\n",
        "\n",
        "print(\"\\n==== Final Evaluation Results ====\")\n",
        "print(f\"Execution Accuracy : {best_exec_acc:.4f}\")\n",
        "print(f\"Average BLEU       : {best_bleu:.4f}\")\n",
        "print(f\"ROUGE-1 F1         : {avg_rouge1:.4f}\")\n",
        "print(f\"ROUGE-2 F1         : {avg_rouge2:.4f}\")\n",
        "print(f\"ROUGE-L F1         : {avg_rougeL:.4f}\")\n",
        "print(f\"Macro Token F1     : {macro_f1:.4f}\")\n",
        "print(f\"Exact Match        : {exact_match:.4f}\")\n",
        "\n",
        "# Save results\n",
        "results_df = pd.DataFrame({\n",
        "    'question': val_questions,\n",
        "    'reference_sql': val_references,\n",
        "    'predicted_sql': best_preds\n",
        "})\n",
        "results_df.to_csv('rag_validation_predictions_best.csv', index=False)\n",
        "print(\"[INFO] Results saved to rag_validation_predictions_best.csv\")\n"
      ],
      "metadata": {
        "id": "4_CIFtIl3LQC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558,
          "referenced_widgets": [
            "008f0eedb647406bbca842f80f14a34a",
            "575a45c6dd4140c58f106b08e2b74863",
            "31f803e710b147c1b51f56050fdcb211",
            "7764d7c3d2874ca78940399f1cde01ce",
            "692a3ec0f105487a8a22fbee7808266b",
            "df0e09ede54c480097ec2757ea10fa9b",
            "7fed757166fd44fc89f691afdc2cb22b",
            "e6e037d5bbf6491894d9cd23807169af",
            "d35019d5d2a04af59c30b9a22ec112de",
            "2a4612c1182145b8b46099e71ae40916",
            "2e7daf8b642f4bd294c0f6729163a123",
            "c69005d87d09451e9676a6584d33a4b7",
            "27db53a4f8864250acd6191623445572",
            "91eb2c8906094b6f92cb4ab62c978506",
            "d5d7d308074b4a74b0114da92efa8c2e",
            "9692499bbda948b4b6c04f9ba7c86cc9",
            "25f776c0f0064df5a6fe84e4c3dcbd2a",
            "1d983024f00d42b68293b9352f083d21",
            "4d13960facf445e38d2a9712486f99e5",
            "87f27a6c7e224024994d7d3d4550b6ab",
            "e2a5d1d2fb6e4744ac2a598a8791c838",
            "4c81a61f960440f6ac4dfda044ec7c56"
          ]
        },
        "outputId": "84e5a8b3-905a-45e3-b444-da874016c18e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using device: cuda\n",
            "[INFO] Loaded model and tokenizer\n",
            "[INFO] Built retrieval corpus with 1678 documents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] FAISS index built with 1678 documents\n",
            "[INFO] Tokenizing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "008f0eedb647406bbca842f80f14a34a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:4006: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1034 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c69005d87d09451e9676a6584d33a4b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrrishabhfree\u001b[0m (\u001b[33mmrrishabhfree-bits-pilani\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250814_044259-0tgdzch6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mrrishabhfree-bits-pilani/huggingface/runs/0tgdzch6' target=\"_blank\">dauntless-oath-19</a></strong> to <a href='https://wandb.ai/mrrishabhfree-bits-pilani/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mrrishabhfree-bits-pilani/huggingface' target=\"_blank\">https://wandb.ai/mrrishabhfree-bits-pilani/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mrrishabhfree-bits-pilani/huggingface/runs/0tgdzch6' target=\"_blank\">https://wandb.ai/mrrishabhfree-bits-pilani/huggingface/runs/0tgdzch6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='483' max='8750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 483/8750 01:22 < 23:38, 5.83 it/s, Epoch 0.55/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VNOHuKxXpXhY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "008f0eedb647406bbca842f80f14a34a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_575a45c6dd4140c58f106b08e2b74863",
              "IPY_MODEL_31f803e710b147c1b51f56050fdcb211",
              "IPY_MODEL_7764d7c3d2874ca78940399f1cde01ce"
            ],
            "layout": "IPY_MODEL_692a3ec0f105487a8a22fbee7808266b"
          }
        },
        "575a45c6dd4140c58f106b08e2b74863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df0e09ede54c480097ec2757ea10fa9b",
            "placeholder": "",
            "style": "IPY_MODEL_7fed757166fd44fc89f691afdc2cb22b",
            "value": "Map:100%"
          }
        },
        "31f803e710b147c1b51f56050fdcb211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6e037d5bbf6491894d9cd23807169af",
            "max": 7000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d35019d5d2a04af59c30b9a22ec112de",
            "value": 7000
          }
        },
        "7764d7c3d2874ca78940399f1cde01ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a4612c1182145b8b46099e71ae40916",
            "placeholder": "",
            "style": "IPY_MODEL_2e7daf8b642f4bd294c0f6729163a123",
            "value": "7000/7000[01:32&lt;00:00,76.04examples/s]"
          }
        },
        "692a3ec0f105487a8a22fbee7808266b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df0e09ede54c480097ec2757ea10fa9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fed757166fd44fc89f691afdc2cb22b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6e037d5bbf6491894d9cd23807169af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d35019d5d2a04af59c30b9a22ec112de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a4612c1182145b8b46099e71ae40916": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e7daf8b642f4bd294c0f6729163a123": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c69005d87d09451e9676a6584d33a4b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27db53a4f8864250acd6191623445572",
              "IPY_MODEL_91eb2c8906094b6f92cb4ab62c978506",
              "IPY_MODEL_d5d7d308074b4a74b0114da92efa8c2e"
            ],
            "layout": "IPY_MODEL_9692499bbda948b4b6c04f9ba7c86cc9"
          }
        },
        "27db53a4f8864250acd6191623445572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25f776c0f0064df5a6fe84e4c3dcbd2a",
            "placeholder": "",
            "style": "IPY_MODEL_1d983024f00d42b68293b9352f083d21",
            "value": "Map:100%"
          }
        },
        "91eb2c8906094b6f92cb4ab62c978506": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d13960facf445e38d2a9712486f99e5",
            "max": 1034,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87f27a6c7e224024994d7d3d4550b6ab",
            "value": 1034
          }
        },
        "d5d7d308074b4a74b0114da92efa8c2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2a5d1d2fb6e4744ac2a598a8791c838",
            "placeholder": "",
            "style": "IPY_MODEL_4c81a61f960440f6ac4dfda044ec7c56",
            "value": "1034/1034[00:13&lt;00:00,75.06examples/s]"
          }
        },
        "9692499bbda948b4b6c04f9ba7c86cc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25f776c0f0064df5a6fe84e4c3dcbd2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d983024f00d42b68293b9352f083d21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d13960facf445e38d2a9712486f99e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87f27a6c7e224024994d7d3d4550b6ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2a5d1d2fb6e4744ac2a598a8791c838": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c81a61f960440f6ac4dfda044ec7c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}